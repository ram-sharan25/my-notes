% Created 2024-09-25 Wed 13:32
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Ram Sharan Rimal  }
\date{\today}
\title{Maximum Likelihood Estimate}
\hypersetup{
 pdfauthor={Ram Sharan Rimal  },
 pdftitle={Maximum Likelihood Estimate},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Maximum Likelihood Estimate}
\label{sec:org74a0272}
\begin{itemize}
\item Split it in two parts
\begin{itemize}
\item The M part( finding the maximum) and the L-part ( which is the likelihood).
\end{itemize}
\item The M part which talks about the maximum of the function is found where its
derivative is zero.
\begin{itemize}
\item It's like climbing the hill, when we are climbing on one, we will get to
top when the inclination ends, which means slope is zero.
\begin{itemize}
\item The slope of function defines its derivative and the point where its
derivative is zero gives the maximum of the function.
\item In case of Gradient Descent we use the same approach but our approach is
to minimize the function.
\end{itemize}
\end{itemize}
\item The L part, the function being maximized is likelihood of seeing the data, given
the model parameters. We tweak the model parameters to maximize the likelihood.
\item It is a systematic way to estimate the true data generating parameters.
\end{itemize}


\begin{itemize}
\item Maxmimum likelihood estimate is one of the optimization algorithm like that
of Mean Squared Error or Sum of Square Errors.

\item As per my understanding of the subject.
\begin{itemize}
\item This is a method to estimate the parameters of the probability distribution
function which explains the data.
\item The idea behind is to find the parameter of the model that maximizes the
likelihood of finding the observed data, assuming that the given model is
best fit for the data.(So it like estimating the observed value of the data
for different values of the parameters that makes up the distribution function.
\end{itemize}
\end{itemize}

\subsection{Likelihood Function:}
\label{sec:org936ea10}
\begin{itemize}
\item The likelihood function represents the probability of observing the data as
a function of model's parameters. If there is are  given data points and a model
with some unknown parameters, then the likelihood function provides how likely
it is to obeserve those points for different values of model paramters.

\item For a set of observations X = \{x1,x2,x3,\ldots{}..xn\}. and a model with parameter
theta , the likelihood function L(theta). is
\end{itemize}


\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-16_23-28-00_screenshot.png}
\end{center}
\begin{itemize}
\item since X is the datasets then the probabilty holds the value of sum of probability
of all the values of X i.e( X ranges from x1 to xn).

\item The main aim of the MLE is to maximize this function. The value of theta for
which this function becomes maximum is the optimal value of theta. And after
fitting this value to the model, it is most likely that model will predict the
observed value correctly.
\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-16_23-38-55_screenshot.png}
\end{center}
\item Since the likelihood function is the product of probabilities then (which can
be very small and tiresome to work with) so that it is common to work with
log of the probabilities of the function to simplify the calculations.

\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-16_23-43-50_screenshot.png}
\end{center}
\item Though we are maximizing the Log Likelihood of the function, it will be same
as maximizing the likelihood function. so that this maximization will result
in the optimal value of theta.

\item Remember that the distribution can be any one among the vast array of distirbution
and the p.d.f can be any from those dist.
\end{itemize}


\subsection{Aggregrating Probabilities in Maximum Likelihood Estimation:}
\label{sec:org15a8599}

\begin{itemize}
\item While we formulate a MLE for the the data with X =\{x1,x2,x3 ,\ldots{}. xn\} with different
probability distribution  we find the combined probrabilities by aggregrating
the probabilities of the individual data points.

\item If the data points are independent to each other then each probabilities are
multiplied to each other to form the aggregrate total probabilities.
\item Which means that the joint probability of the dataset  i.e likelihood of observing
the entire dataset is the product of individual probabilities of each observation.

\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-17_23-13-53_screenshot.png}
\end{center}

\item The reason of multiplication is due to the fact that individual probalities are independent
to each other.

To calculate the joint probabilities of two independent probablities we get a product of them.
     as ,
\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-17_23-16-31_screenshot.png}
\end{center}
\item As the single value of probabilities is very small and multiplying such small values results
in very very smaller values , so that log of the joint probabilities is taken for
easier calcuation and understanding.

\item The example of maximum likelihood estimation for Normal distribution.
\end{itemize}
\section{Maximum Likelihood for Normal Distibution}
\label{sec:org37b6a64}
\begin{itemize}
\item Lets say we have X = \{x1,x2,\ldots{}..xn\} points which is defined by a Normal distri
bution with parameters N(mean,sigma\textsuperscript{2}) where mean is the mean of all the data
and sigma\textsuperscript{2} is the variance of the data.
\item The PDF for Normal distribution is defined as

\item Now the likelihood function for the Normal distribution is
\end{itemize}


\begin{itemize}
\item Now, taking the log likelihood of the function we get ,
\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-17_00-17-16_screenshot.png}
\end{center}
\item Notice how the multiplication changes after applying log.
\end{itemize}

\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-17_00-18-10_screenshot.png}
\end{center}
\href{Maximum Likelihood Estimate.org}{Maximum Likelihood Estimate}

\begin{itemize}
\item The above function defines the log likelihood of data points X w.r.t mean and sigma\textsuperscript{2} of
the data

\item Now the second step is to maximize this log likelihood so that the optimal value of para
meters can be found, and the best likelihood estimate of data can be estimated.
\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-17_12-40-02_screenshot.png}
\end{center}
\end{itemize}


\subsection{Maximum Likelihood for Bernoulli Distribution}
\label{sec:orgf623c52}

\begin{itemize}
\item The Bernoulli Distribution is used to model the distribution which has two independent
events in it. One is success(denoted by 1) and another is failure (denoted by 0).

\item Probability Mass Function for discrete random variable
\item Probability Density function for continuous random variable.
\end{itemize}


\begin{itemize}
\item The probability mass  function for Bernoulli distribution is:

\begin{center}
\includegraphics[width=600cm]{data/Maximum Likelihood Estimate/2024-09-24_23-17-17_screenshot.png}
\end{center}
\end{itemize}
\end{document}
